# ðŸ§  RAG API â€” Retrieval-Augmented Generation System

This project implements a **Dockerized RAG (Retrieval-Augmented Generation) API** designed to run efficiently on **CPU-only machines** and handle **100,000+ documents** for question answering. The system uses a modular microservice structure with the following components:

- **API Service (`api/`)** â€” Hosts endpoints to upload documents and generate answers.
- **Embedding Service (`embedding/`)** â€” Converts text into vector embeddings using a sentence-transformer model.
- **LLM Generation Service (`ollama/`)** â€” Uses a local language model (e.g., Qwen2.5-0.5B) via Ollama.
- **Vector Store (`vector_store/`)** â€” Stores embeddings using ChromaDB.

---

## ðŸš€ Features

- Upload and index large-scale documents (100k+ with ~5k characters each)
- Query documents with contextual responses from a local LLM
- Optimized for CPU-only environments (<16GB RAM)
- Follows MLOps and modular coding best practices
- Scored with RAGAS metrics for precision and relevance

---

## ðŸ§© API Endpoints

### 1. `GET /` â€” API Status Check
```json
{
  "message": "RAG API is running successfully"
}
```

### 2. `POST /upload`
Uploads documents and stores vector embeddings.
#### Request:
```json
{
  "texts": [
    "Document 1 text...",
    "Document 2 text..."
  ]
}
```
#### Response:
```json
{
  "message": "Vector index successfully created",
  "nodes_count": 123
}
```

### 3. `POST /generate`
Generates an answer based on document context.
#### Request:
```json
{
  "new_message": {
    "role": "user",
    "content": "What is the capital of France?"
  }
}
```
#### Response:
```json
{
  "generated_text": "The capital of France is Paris.",
  "contexts": [
    "Paris is the capital of France. It is known for the Eiffel Tower."
  ]
}
```

---

## ðŸ³ Docker Setup

### Build & Run
```bash
docker-compose up --build
```

### Container Roles
- `api` â€” FastAPI service running `api_rag.py`
- `embedding` â€” Embedding service using `embed_server.py`
- `ollama` â€” Runs local LLM using `start.sh` script

---

## ðŸ§ª Testing

Run the provided test script to verify API:
```bash
python test_api.py
```
This checks all endpoints: status, upload, and generate.

---

## ðŸ“ Project Structure
```
RAG-API/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ api_rag.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ embedding/
â”‚   â”œâ”€â”€ embed_server.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ start.sh
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ vector_store/
â”‚   â””â”€â”€ chroma.db
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ test_api.py
â””â”€â”€ README.md
```

---

## âœ… Requirements

- Docker v28.0.1
- Docker Compose v2.33.1
- CPU-only machine (â‰¤16GB RAM)

---

## ðŸ“¦ Submission
Submit `group_X.zip` including:
- All code and configuration files
- `README.md`, `requirements.txt`, and scripts
- `docker-compose.yaml`

---

